{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1zN6fdc5kvC-1SAGdRh1R1fmExxnYtfgc","authorship_tag":"ABX9TyN15Y7uVXciYlzMoSBqrcr6"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["!pip install rarfile\n","\n","import rarfile\n","import os\n","\n","# Specify the path to your rar file and the extraction directory\n","rar_path = \"/content/deepfake_sample_dataset.rar\"  # Note: I assume the rar file is at /content/deepfake_sample_dataset.rar\n","extract_path = \"/content/dataset_rar\"\n","\n","# Create the extraction directory if it doesn't exist\n","os.makedirs(extract_path, exist_ok=True)\n","\n","# Open and extract the rar file\n","try:\n","    with rarfile.RarFile(rar_path, 'r') as rf:\n","        rf.extractall(extract_path)\n","    print(f\"Successfully extracted {rar_path} to {extract_path}\")\n","except rarfile.Error as e:\n","    print(f\"Error extracting rar file: {e}\")\n","except FileNotFoundError:\n","    print(f\"Error: The file {rar_path} was not found.\")\n","\n","# Check if it worked\n","if os.path.exists(extract_path):\n","    print(\"Extracted contents:\")\n","    print(os.listdir(extract_path))\n","else:\n","    print(\"Extraction path does not exist.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"I1cavFow4_D-","executionInfo":{"status":"ok","timestamp":1754458922139,"user_tz":-330,"elapsed":7158,"user":{"displayName":"Akilesh S","userId":"07458336900204139617"}},"outputId":"a9608af9-53ed-476d-d099-73dd7dd5b456"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: rarfile in /usr/local/lib/python3.11/dist-packages (4.2)\n","Successfully extracted /content/deepfake_sample_dataset.rar to /content/dataset_rar\n","Extracted contents:\n","['deepfake_sample_dataset']\n"]}]},{"cell_type":"code","source":["!pip install gradio --quiet\n"],"metadata":{"id":"cmx3sqX95n51","executionInfo":{"status":"ok","timestamp":1754459675216,"user_tz":-330,"elapsed":8241,"user":{"displayName":"Akilesh S","userId":"07458336900204139617"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["import os\n","from torchvision import datasets, transforms\n","from torch.utils.data import DataLoader\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torchvision.models as models\n","\n","# 1. Set up data transforms\n","transform = transforms.Compose([\n","    transforms.Resize((224, 224)),\n","    transforms.ToTensor(),\n","])\n","\n","# 2. Load the dataset\n","# Update the data_dir to point to the extracted dataset\n","data_dir = \"/content/dataset_rar/deepfake_sample_dataset\"\n","dataset = datasets.ImageFolder(root=data_dir, transform=transform)\n","data_loader = DataLoader(dataset, batch_size=16, shuffle=True)\n","\n","# 3. Setup the model (Transfer Learning)\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model = models.resnet18(pretrained=True)\n","\n","# Replace final layer for 3 classes (real, ai, drawn)\n","model.fc = nn.Linear(model.fc.in_features, 3)\n","model.to(device)\n","\n","# 4. Train the model\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=0.001)\n","\n","# 5. Training loop (small demo for 5 epochs)\n","for epoch in range(5):\n","    running_loss = 0.0\n","    for inputs, labels in data_loader:\n","        inputs, labels = inputs.to(device), labels.to(device)\n","\n","        optimizer.zero_grad()\n","        outputs = model(inputs)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","\n","        running_loss += loss.item()\n","    print(f\"Epoch {epoch+1}, Loss: {running_loss:.4f}\")\n","\n","# 6. Save the trained model\n","torch.save(model.state_dict(), \"/content/deepfake_model.pth\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UcbQ0hVA56lk","executionInfo":{"status":"ok","timestamp":1754459102747,"user_tz":-330,"elapsed":13615,"user":{"displayName":"Akilesh S","userId":"07458336900204139617"}},"outputId":"3ce7b511-e82f-4765-f3cc-c8d1e8652547"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n","Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n","100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 44.7M/44.7M [00:00<00:00, 77.7MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1, Loss: 1.2580\n","Epoch 2, Loss: 0.0142\n","Epoch 3, Loss: 0.0019\n","Epoch 4, Loss: 0.0014\n","Epoch 5, Loss: 0.0008\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"c5fd280a","executionInfo":{"status":"ok","timestamp":1754459170201,"user_tz":-330,"elapsed":412,"user":{"displayName":"Akilesh S","userId":"07458336900204139617"}},"outputId":"1712c45c-22e7-48b0-ac4c-d777e0904bc5"},"source":[],"execution_count":9,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n","  warnings.warn(msg)\n"]},{"output_type":"stream","name":"stdout","text":["Using test image: /content/dataset_rar/deepfake_sample_dataset/real/image_2.jpg\n","The predicted class for the image is: real\n"]}]},{"cell_type":"code","source":["import gradio as gr\n","\n","# Ensure model is in eval mode\n","model.eval()\n","\n","def classify_image(img):\n","    from PIL import Image\n","    import torch\n","    import torchvision.transforms as transforms\n","\n","    transform = transforms.Compose([\n","        transforms.Resize((224, 224)),\n","        transforms.ToTensor(),\n","    ])\n","\n","    img = img.convert(\"RGB\")\n","    img = transform(img).unsqueeze(0).to(device)\n","\n","    with torch.no_grad():\n","        output = model(img)\n","        pred = torch.argmax(output, dim=1).item()\n","\n","    return dataset.classes[pred]\n"],"metadata":{"id":"eDNvlj1I6mK9","executionInfo":{"status":"ok","timestamp":1754459795622,"user_tz":-330,"elapsed":5807,"user":{"displayName":"Akilesh S","userId":"07458336900204139617"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["interface = gr.Interface(\n","    fn=classify_image,\n","    inputs=gr.Image(type=\"pil\"),\n","    outputs=\"label\",\n","    title=\"üîç DeepFake Detection\",\n","    description=\"Upload an image to detect if it's REAL, AI-generated, or HAND-DRAWN.\",\n","    examples=[\n","        [\"/content/dataset/real/sample1.jpg\"],  # Replace with real paths\n","        [\"/content/dataset/ai/sample2.jpg\"],\n","        [\"/content/dataset/drawn/sample3.jpg\"]\n","    ]\n",")\n","\n","interface.launch(share=True)\n"],"metadata":{"executionInfo":{"status":"ok","timestamp":1754459876958,"user_tz":-330,"elapsed":2917,"user":{"displayName":"Akilesh S","userId":"07458336900204139617"}},"colab":{"base_uri":"https://localhost:8080/","height":611},"id":"3ViLo_Cv85wX","outputId":"33319300-11fc-4bf8-86fc-cd87b35eb08a"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n","* Running on public URL: https://48aa4e64202ff2ce12.gradio.live\n","\n","This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<div><iframe src=\"https://48aa4e64202ff2ce12.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"]},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":[]},"metadata":{},"execution_count":13}]},{"cell_type":"code","source":["import gradio as gr\n","from PIL import Image\n","import torch\n","import torchvision.transforms as transforms\n","import torchvision.models as models\n","import torch.nn as nn\n","\n","# Load the saved model (assuming it's already loaded in the environment)\n","# If not, you would need to add code here to load it.\n","# Example:\n","# model_path = \"/content/deepfake_model.pth\"\n","# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","# model = models.resnet18(pretrained=False)\n","# model.fc = nn.Linear(model.fc.in_features, 3)\n","# model.load_state_dict(torch.load(model_path, map_location=device))\n","# model.to(device)\n","model.eval() # Ensure model is in eval mode\n","\n","def classify_image(img: Image.Image) -> str:\n","    \"\"\"\n","    Classifies an uploaded image as real, AI-generated, or drawn and returns a descriptive paragraph.\n","\n","    Args:\n","        img: The uploaded image as a PIL Image object.\n","\n","    Returns:\n","        A string containing a short paragraph describing the classification.\n","    \"\"\"\n","    transform = transforms.Compose([\n","        transforms.Resize((224, 224)),\n","        transforms.ToTensor(),\n","    ])\n","\n","    img = img.convert(\"RGB\")\n","    img = transform(img).unsqueeze(0).to(device)\n","\n","    with torch.no_grad():\n","        output = model(img)\n","        pred = torch.argmax(output, dim=1).item()\n","\n","    # Assuming the order is the same as during training\n","    class_labels = ['ai', 'drawn', 'real']\n","    predicted_class = class_labels[pred]\n","\n","    # Generate the descriptive paragraph based on the prediction\n","    if predicted_class == 'real':\n","        return \"Based on the analysis, this image appears to be a real photograph captured by a camera.\"\n","    elif predicted_class == 'ai':\n","        return \"Based on the analysis, this image is likely AI-generated. It exhibits characteristics often found in images created by artificial intelligence models.\"\n","    elif predicted_class == 'drawn':\n","        return \"Based on the analysis, this image seems to be hand-drawn or created using drawing tools. It displays traits typical of artistic creations.\"\n","    else:\n","        return \"Unable to classify the image.\"\n","\n","\n","interface = gr.Interface(\n","    fn=classify_image,\n","    inputs=gr.Image(type=\"pil\"),\n","    outputs=\"text\",  # Change output type to \"text\" for paragraph output\n","    title=\"üîç DeepFake Detection\",\n","    description=\"Upload an image to detect if it's REAL, AI-generated, or HAND-DRAWN and get a descriptive analysis.\",\n","    examples=[\n","        [\"/content/dataset_rar/deepfake_sample_dataset/real/image_1.jpg\"],\n","        [\"/content/dataset_rar/deepfake_sample_dataset/ai/image_1.jpg\"],\n","        [\"/content/dataset_rar/deepfake_sample_dataset/drawn/image_1.jpg\"]\n","    ]\n",")\n","\n","interface.launch(share=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":611},"id":"EJ6PhX6Y9SGm","executionInfo":{"status":"ok","timestamp":1754461978512,"user_tz":-330,"elapsed":868,"user":{"displayName":"Akilesh S","userId":"07458336900204139617"}},"outputId":"1890fba7-294c-47c7-edbd-6add6a887813"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n","* Running on public URL: https://66b068deb4a4a7172a.gradio.live\n","\n","This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<div><iframe src=\"https://66b068deb4a4a7172a.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"]},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":[]},"metadata":{},"execution_count":15}]},{"cell_type":"code","source":["import os\n","\n","# Define the project directory\n","project_dir = \"/content/deepfake_detection_project\"\n","os.makedirs(project_dir, exist_ok=True)\n","\n","# Create a directory for the model\n","model_dir = os.path.join(project_dir, \"model\")\n","os.makedirs(model_dir, exist_ok=True)\n","\n","# Create a directory for the dataset (optional, if you want to include a sample)\n","# dataset_dir = os.path.join(project_dir, \"dataset\")\n","# os.makedirs(dataset_dir, exist_ok=True)\n","\n","# Save the trained model file\n","# Assuming the model was saved at /content/deepfake_model.pth\n","model_path_colab = \"/content/deepfake_model.pth\"\n","model_path_project = os.path.join(model_dir, \"deepfake_model.pth\")\n","if os.path.exists(model_path_colab):\n","    with open(model_path_colab, 'rb') as f_in, open(model_path_project, 'wb') as f_out:\n","        f_out.write(f_in.read())\n","    print(f\"Model file saved to {model_path_project}\")\n","else:\n","    print(f\"Model file not found at {model_path_colab}. Skipping model save.\")\n","\n","\n","# Create the Python file for the Gradio app\n","gradio_app_code = \"\"\"\n","import gradio as gr\n","from PIL import Image\n","import torch\n","import torchvision.transforms as transforms\n","import torchvision.models as models\n","import torch.nn as nn\n","import os\n","\n","# Define the device\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Load the saved model\n","# Assuming the model file is in the 'model' subfolder\n","model_path = \"model/deepfake_model.pth\"\n","\n","# Load the same model architecture used for training\n","model = models.resnet18(pretrained=False)\n","model.fc = nn.Linear(model.fc.in_features, 3)\n","\n","# Load the state dict, handling potential missing keys if architecture differs slightly\n","try:\n","    model.load_state_dict(torch.load(model_path, map_location=device))\n","except RuntimeError as e:\n","    print(f\"Error loading model state_dict: {e}\")\n","    print(\"Attempting to load with strict=False...\")\n","    model.load_state_dict(torch.load(model_path, map_location=device), strict=False)\n","\n","\n","model.to(device)\n","model.eval() # Set the model to evaluation mode\n","\n","# Define the same transforms used for training\n","transform = transforms.Compose([\n","    transforms.Resize((224, 224)),\n","    transforms.ToTensor(),\n","])\n","\n","def classify_image(img: Image.Image) -> str:\n","    \\\"\\\"\\\"\n","    Classifies an uploaded image as real, AI-generated, or drawn and returns a descriptive paragraph.\n","\n","    Args:\n","        img: The uploaded image as a PIL Image object.\n","\n","    Returns:\n","        A string containing a short paragraph describing the classification.\n","    \\\"\\\"\\\"\n","    img = img.convert(\"RGB\")\n","    img = transform(img).unsqueeze(0).to(device)\n","\n","    with torch.no_grad():\n","        output = model(img)\n","        pred = torch.argmax(output, dim=1).item()\n","\n","    # Assuming the order is the same as during training\n","    # It's best to get the class labels from the dataset if possible,\n","    # but for this example, we'll use the assumed order.\n","    class_labels = ['ai', 'drawn', 'real']\n","    predicted_class = class_labels[pred]\n","\n","    # Generate the descriptive paragraph based on the prediction\n","    if predicted_class == 'real':\n","        return \"Based on the analysis, this image appears to be a real photograph captured by a camera.\"\n","    elif predicted_class == 'ai':\n","        return \"Based on the analysis, this image is likely AI-generated. It exhibits characteristics often found in images created by artificial intelligence models.\"\n","    elif predicted_class == 'drawn':\n","        return \"Based on the analysis, this image seems to be hand-drawn or created using drawing tools. It displays traits typical of artistic creations.\"\n","    else:\n","        return \"Unable to classify the image.\"\n","\n","# Example usage (optional, could be removed for the final app.py)\n","# if __name__ == \"__main__\":\n","#     # Example of how to use the function with a local image file\n","#     # Make sure to have a test image available in your project structure\n","#     test_image_path = \"path/to/your/test_image.jpg\" # Replace with a valid path\n","#     if os.path.exists(test_image_path):\n","#         prediction = classify_image(Image.open(test_image_path))\n","#         print(f\"Prediction for {test_image_path}: {prediction}\")\n","#     else:\n","#         print(f\"Test image not found at {test_image_path}\")\n","\n","\n","interface = gr.Interface(\n","    fn=classify_image,\n","    inputs=gr.Image(type=\"pil\"),\n","    outputs=\"text\",\n","    title=\"üîç DeepFake Detection\",\n","    description=\"Upload an image to detect if it's REAL, AI-generated, or HAND-DRAWN and get a descriptive analysis.\",\n","    # Examples would need to be paths relative to where the app is run,\n","    # or you would need to include sample images in your project structure.\n","    # For simplicity in deployment, examples might be omitted or handled differently.\n","    # examples=[\n","    #     [\"path/to/sample_real.jpg\"],\n","    #     [\"path/to/sample_ai.jpg\"],\n","    #     [\"path/to/sample_drawn.jpg\"]\n","    # ]\n",")\n","\n","if __name__ == \"__main__\":\n","    interface.launch()\n","\n","\"\"\"\n","\n","gradio_app_file = os.path.join(project_dir, \"app.py\")\n","with open(gradio_app_file, \"w\") as f:\n","    f.write(gradio_app_code)\n","print(f\"Gradio app code saved to {gradio_app_file}\")\n","\n","# Create a requirements.txt file\n","requirements_content = \"\"\"\n","torch\n","torchvision\n","gradio\n","Pillow\n","rarfile # If you need to handle rar files in your local environment\n","\"\"\"\n","\n","requirements_file = os.path.join(project_dir, \"requirements.txt\")\n","with open(requirements_file, \"w\") as f:\n","    f.write(requirements_content)\n","print(f\"Requirements file saved to {requirements_file}\")\n","\n","print(\"\\nProject structure created. You can download the folder:\")\n","print(project_dir)\n","print(\"To run the Gradio app locally:\")\n","print(f\"1. Navigate to the project directory: cd {project_dir}\")\n","print(\"2. Install dependencies: pip install -r requirements.txt\")\n","print(\"3. Run the app: python app.py\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OwKM--yAEBFD","executionInfo":{"status":"ok","timestamp":1754462630846,"user_tz":-330,"elapsed":205,"user":{"displayName":"Akilesh S","userId":"07458336900204139617"}},"outputId":"fb4ae602-c0e9-405d-d7be-31eed1721605"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["Model file saved to /content/deepfake_detection_project/model/deepfake_model.pth\n","Gradio app code saved to /content/deepfake_detection_project/app.py\n","Requirements file saved to /content/deepfake_detection_project/requirements.txt\n","\n","Project structure created. You can download the folder:\n","/content/deepfake_detection_project\n","To run the Gradio app locally:\n","1. Navigate to the project directory: cd /content/deepfake_detection_project\n","2. Install dependencies: pip install -r requirements.txt\n","3. Run the app: python app.py\n"]}]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"RGnaLPUQIYKI"},"execution_count":null,"outputs":[]}]}